{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\kyabr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\kyabr\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark import SparkConf, SparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up spark environment. Make any changes... I added a bunch of stuff ot try and get rdds going but nothing worked for me!\n",
    "Fyi : to see spark on browser, just enter http://localhost:4040 into browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"Movie Review Analysis\") \\\n",
    "    .set(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "    .set(\"spark.python.worker.reuse\", \"true\") \\\n",
    "    .set(\"spark.python.worker.timeout\", \"1800\") \\\n",
    "    .set(\"spark.network.timeout\", \"1800\") \\\n",
    "    .set(\"spark.python.daemon.module\", \"pyspark.daemon\") \\\n",
    "    .set(\"spark.python.worker.memory\", \"4g\") \\\n",
    "    .set(\"spark.executor.instances\", \"2\") \\\n",
    "    .set(\"spark.driver.memory\", \"4g\")\\\n",
    "\n",
    "# Create or get the existing Spark session with the provided configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading and \"transformaing\" data (this could be changed a ton so go ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieReview_df = spark.read.option(\"header\", \"true\").csv(\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieReview_df = movieReview_df.withColumnRenamed('class_index', 'rating') \\\n",
    "                               .withColumnRenamed('_c1', 'review_title') \\\n",
    "                               .withColumnRenamed('_c2', 'review_text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|rating|        review_title|         review_text|\n",
      "+------+--------------------+--------------------+\n",
      "|     2|            Great CD|\"My lovely Pat ha...|\n",
      "|     2|One of the best g...|Despite the fact ...|\n",
      "|     1|Batteries died wi...|I bought this cha...|\n",
      "|     2|works fine, but M...|Check out Maha En...|\n",
      "|     2|Great for the non...|Reviewed quite a ...|\n",
      "|     1|DVD Player crappe...|I also began havi...|\n",
      "|     1|      Incorrect Disc|I love the style ...|\n",
      "|     1|DVD menu select p...|I cannot scroll t...|\n",
      "|     2|Unique Weird Orie...|\"Exotic tales of ...|\n",
      "|     1|\"Not an \"\"ultimat...|Firstly,I enjoyed...|\n",
      "|     2|Great book for tr...|I currently live ...|\n",
      "|     1|                Not!|If you want to li...|\n",
      "|     1|     A complete Bust|\"This game requir...|\n",
      "|     2|TRULY MADE A DIFF...|I have been using...|\n",
      "|     1|didn't run off of...|Was hoping that t...|\n",
      "|     1|          Don't buy!|First of all, the...|\n",
      "|     2|Simple, Durable, ...|This is an AWESOM...|\n",
      "|     2|Review of Kelly C...|For the price of ...|\n",
      "|     2|SOY UN APASIONADO...|Y ESTE LIBRO ESTÃ...|\n",
      "|     2|Some of the best ...|This is an excell...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieReview_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rating: string (nullable = true)\n",
      " |-- review_title: string (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieReview_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicated detected\n"
     ]
    }
   ],
   "source": [
    "if movieReview_df.count() > movieReview_df.dropDuplicates(['review_title', 'review_text']).count():\n",
    "    print(\"duplicated detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieReview_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399996"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieReview_df = movieReview_df.dropDuplicates(['review_title','review_text'])\n",
    "movieReview_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|rating| count|\n",
      "+------+------+\n",
      "|     1|199996|\n",
      "|     2|200000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieReview_df.groupBy('rating').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.parsing.preprocessing as gsp\n",
    "from pyspark.sql.types import StringType\n",
    "from gensim import utils\n",
    "\n",
    "filters = [\n",
    "    gsp.strip_tags,\n",
    "    gsp.strip_punctuation,\n",
    "    gsp.strip_multiple_whitespaces,\n",
    "    gsp.strip_numeric,\n",
    "    gsp.remove_stopwords,\n",
    "    gsp.strip_short,\n",
    "    gsp.stem_text\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removed review titles somewhere, we should probably keep it in so we can add it to review text and ahve one long line of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------------------+\n",
      "|rating|review_title|         review_text|\n",
      "+------+------------+--------------------+\n",
      "|     1|        NULL|I can't give you ...|\n",
      "|     1|        NULL|I ordered a 12 fo...|\n",
      "+------+------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieReview_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- START OF TODO: MAKE THIS RDD STUFF ACTUALLY WORK!!! --------------\n",
    "I am trying to make it so we do the majority of cleaning through rdds, just for assignment reasons\n",
    "So if someone could please try to do that, even with the cleaning I did above!\n",
    "For example, i think there is a function called \"distinct\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    # Get the review title and review text\n",
    "    title = x[1]\n",
    "    review_text = x[2]\n",
    "\n",
    "    # Concatenate the title and review text\n",
    "    combined_text = title + \" \" + review_text  # Add a space between title and text\n",
    "\n",
    "    # Convert to lowercase and clean the combined text\n",
    "    combined_text = combined_text.lower()\n",
    "    combined_text = utils.to_unicode(combined_text)\n",
    "\n",
    "    # Apply filters to the combined text\n",
    "    for f in filters:\n",
    "        combined_text = f(combined_text)\n",
    "\n",
    "    return (x[0], combined_text)  # Return the ID and the cleaned combined text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movieReview_df = movieReview_df.limit(10)\n",
    "input_rdd = movieReview_df.rdd.map(lambda x : clean_text(x))\n",
    "input_rdd = movieReview_df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idea: join review text to review title, instead of getting rid of review title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_rdd = input_rdd.repartition(100)  # Adjust the number of partitions as needed\n",
    "#input_rdd = input_rdd.distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(input_rdd.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_df = input_rdd.toDF(['rating', 'review_title','review_text'])\n",
    "#input_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- END OF TODO --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just me manually cleaning, but we should rlly do it using rdd and map functions if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Your original cleaning function, adapted to work as a UDF\n",
    "def clean_text(review_text):\n",
    "    # Handle NULL or missing text\n",
    "    if review_text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    review_text = review_text.lower()\n",
    "    \n",
    "    # Remove special characters, numbers, and excessive punctuation\n",
    "    review_text = re.sub(r'[^\\w\\s]', '', review_text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    review_text = re.sub(r'\\s+', ' ', review_text).strip()\n",
    "    \n",
    "    # Remove stop words (optional: using a stop words remover)\n",
    "    stop_words = set([\"the\", \"a\", \"an\", \"and\", \"is\", \"it\", \"to\", \"you\", \"for\", \"in\", \"of\", \"on\", \"that\", \"this\"])\n",
    "    review_text = ' '.join([word for word in review_text.split() if word not in stop_words])\n",
    "    \n",
    "    return review_text\n",
    "# Register the UDF\n",
    "clean_review_udf = udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function to the 'text_column' and create a new column with the cleaned text\n",
    "df_cleaned = movieReview_df.withColumn('cleaned_review_text', clean_review_udf(movieReview_df['review_text']))\n",
    "\n",
    "# Optionally, drop the original column and keep only the cleaned column\n",
    "df_cleaned = movieReview_df.drop('text_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------------------+\n",
      "|rating|review_title|         review_text|\n",
      "+------+------------+--------------------+\n",
      "|     1|        NULL|I can't give you ...|\n",
      "+------+------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399996\n"
     ]
    }
   ],
   "source": [
    "print(movieReview_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrText_df = movieReview_df.drop('reivew_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "+------+--------------------+--------------------+\n",
      "|rating|        review_title|         review_text|\n",
      "+------+--------------------+--------------------+\n",
      "|     1|                   !|This is a great b...|\n",
      "|     0|! Concise && ( no...|If you want to lo...|\n",
      "|     0|      !!! URGHHH !!!|This book was BOR...|\n",
      "|     1|                !!!!|What can i say? I...|\n",
      "|     0|!!!Not enough to ...|Make sure you che...|\n",
      "|     0|             \"\"\" \"\"\"|\"I like the song ...|\n",
      "|     0|\"\"\" Smile\"\".........|\"To those of us t...|\n",
      "|     0|         \"\"\"1\"\" star| because 1/5 of t...|\n",
      "|     1|\"\"\"A Christmas St...|I absolutely love...|\n",
      "|     0|\"\"\"A Hole Too Sma...|\"The seat is beau...|\n",
      "|     1|\"\"\"A MUST HAVE\"\" ...|I would call this...|\n",
      "|     1|   \"\"\"A Must Have\"\"\"|If you don't know...|\n",
      "|     1|\"\"\"A Very Excitin...|Benno Thomas has ...|\n",
      "|     1|\"\"\"ALFA & ZULU\"\" ...|\"Thanks to K6YB f...|\n",
      "|     0|         \"\"\"Alive\"\"\"|Go P.O.D way to l...|\n",
      "|     0|\"\"\"Along the stre...|This book came hi...|\n",
      "|     0|\"\"\"American\"\" Law...|I really give it ...|\n",
      "|     1|\"\"\"An Exciting Re...|An excellent take...|\n",
      "|     1|\"\"\"Another dude i...|Yes, he is anothe...|\n",
      "|     1|           \"\"\"Apt\"\"\"|The best thing ab...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Drop rows with missing values\n",
    "mrText_df = mrText_df.dropna()\n",
    "\n",
    "# (Optional: Limit for testing purposes)\n",
    "mrText_df = mrText_df.limit(10000)  # Remove this after testing\n",
    "\n",
    "\n",
    "# Convert 'rating' column to integer\n",
    "mrText_df = mrText_df.withColumn(\"rating\", col(\"rating\").cast(\"int\"))\n",
    "\n",
    "# Map 'rating' values: 2 -> 1, 1 -> 0\n",
    "mrText_df = mrText_df.withColumn(\"rating\", when(col(\"rating\") == 2, 1).when(col(\"rating\") == 1, 0))\n",
    "\n",
    "# Verify the transformation\n",
    "print(mrText_df.count())  # Count the number of rows\n",
    "mrText_df.show()          # Display the updated DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF CLEANING -- ON TO TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = mrText_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Rename or drop the existing 'words' column if it exists\n",
    "if \"words\" in train_df.columns:\n",
    "    train_df = train_df.drop(\"words\")\n",
    "if \"features\" in train_df.columns:\n",
    "    train_df = train_df.drop(\"features\")\n",
    "    \n",
    "\n",
    "# Define tokenizer and Word2Vec\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")  # Ensure no conflicts\n",
    "w2v = Word2Vec(vectorSize=300, minCount=0, inputCol=\"words\", outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec2vec_pipeline = Pipeline(stages=[tokenizer, w2v])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m doc2vec_model \u001b[38;5;241m=\u001b[39m \u001b[43mdec2vec_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(10061, 'No connection could be made because the target machine actively refused it', None, 10061, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:2179\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2176\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m   2177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[1;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[0;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[1;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\kyabr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "doc2vec_model = dec2vec_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = doc2vec_model.transform(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "never ended up getting the train_df, test_df transform functions running. 0 clue why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df = doc2vec_model.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few rows from train df:\n",
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|rating|        review_title|         review_text|               words|            features|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     0|! Concise && ( no...|If you want to lo...|[if, you, want, t...|[-0.0464008737425...|\n",
      "|     0|      !!! URGHHH !!!|This book was BOR...|[this, book, was,...|[-0.0472781334912...|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show a few rows from the train DataFrame\n",
    "print(\"Few rows from train df:\")\n",
    "train_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_df.select(\"features\").collect()\n",
    "train_labels = train_df.select(\"rating\").collect()\n",
    "test_features = test_df.select(\"features\").collect()\n",
    "test_labels = test_df.select(\"rating\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray([v[0].toArray() for v in train_features])\n",
    "Y_train = np.asarray([v[0] for v in train_labels])\n",
    "X_test = np.asarray([v[0].toArray() for v in train_features])\n",
    "Y_test = np.asarray([v[0] for v in train_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbClassifier = xgb.XGBClassifier(max_depth=10, seed = 18238, objective = 'multi:softmax', num_class=2)\n",
    "model = xgbClassifier.fit(X_train, Y_train)\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score is: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "auc_scores = cross_val_score(model, X, Y, cv=5, scoring='roc_auc')\n",
    "print(\"Cross-validated AUC scores:\", auc_scores)\n",
    "print(\"Mean AUC score:\", auc_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAJOR ISSUE HERE: we get an accuracy of 100% ... which is probably not possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
